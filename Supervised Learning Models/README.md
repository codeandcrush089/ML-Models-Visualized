
## 1. **Linear Regression**

### ğŸ“˜ Description :-

Linear Regression is a supervised learning algorithm that models the relationship between input features and a continuous target variable by fitting a best-fit straight line. Itâ€™s primarily used when data shows a linear correlation between independent and dependent variables.


### âš™ï¸ **Key Points**

* **Type:** Regression
* **Output Type:** Continuous
* **Algorithm / Technique:** Ordinary Least Squares (OLS)
* **Best Use Case:** Predicting numeric outcomes such as prices, revenue, or growth trends.


### ğŸ’» **Code Example**

```python
from sklearn.linear_model import LinearRegression
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)
model = LinearRegression().fit(X_train, y_train)
print("RÂ² Score:", model.score(X_test, y_test))
```

ğŸ‘‰ [Try it on Colab](https://colab.research.google.com/)


###  **ğŸ“Š Diagram / Workflow Prompt**

<img src="https://github.com/codeandcrush089/ML-Models-Visualized/blob/main/Img/Linear%20Regression%201.JPG" width="600">

### ğŸ“ˆ **Pros & Cons**

* âœ… Simple and highly interpretable
* âœ… Fast and efficient for small to medium datasets
* âš ï¸ Performs poorly with non-linear or highly correlated features


### ğŸ“š **Real-World Use Case**

ğŸ  Used in **house price prediction**, **sales forecasting**, and **trend analysis** where relationships between variables are approximately linear.


---

## 2 **Ridge Regression**

### ğŸ“˜ Description

Ridge Regression is an advanced form of Linear Regression that adds **L2 regularization** to penalize large coefficients. Itâ€™s used to prevent overfitting and handle multicollinearity in regression models.


### âš™ï¸ **Key Points**

* **Type:** Regression
* **Output Type:** Continuous
* **Algorithm / Technique:** L2 Regularization (Tikhonov Regularization)
* **Best Use Case:** When features are highly correlated or when overfitting is observed in linear models.


### ğŸ’» **Code Example**

```python
from sklearn.linear_model import Ridge
model = Ridge(alpha=1.0)
model.fit(X_train, y_train)
print("RÂ² Score:", model.score(X_test, y_test))
```

ğŸ‘‰ [Try it on Colab](https://colab.research.google.com/)


### ğŸ“Š **Diagram / Workflow Prompt**

<img src="https://github.com/codeandcrush089/ML-Models-Visualized/blob/main/Img/Ridge%20Regression%201.JPG" width="600">


### ğŸ“ˆ **Pros & Cons**

* âœ… Reduces overfitting by shrinking coefficients
* âœ… Works well with multicollinearity
* âš ï¸ Doesnâ€™t perform automatic feature selection (all coefficients remain non-zero)


### ğŸ“š **Real-World Use Case**

ğŸ“Š Used in **financial forecasting**, **healthcare cost prediction**, and **energy demand estimation** where features are interrelated.


---

## 3 **Lasso Regression**

### ğŸ“˜ Description

Lasso Regression adds **L1 regularization** to the Linear Regression model, penalizing the absolute value of coefficients. Itâ€™s mainly used for **feature selection** by driving irrelevant feature coefficients to zero.


### âš™ï¸ **Key Points**

* **Type:** Regression
* **Output Type:** Continuous
* **Algorithm / Technique:** L1 Regularization
* **Best Use Case:** When you need both prediction and automatic feature selection.


### ğŸ’» **Code Example**

```python
from sklearn.linear_model import Lasso
model = Lasso(alpha=0.1)
model.fit(X_train, y_train)
print("RÂ² Score:", model.score(X_test, y_test))
```

ğŸ‘‰ [Try it on Colab](https://colab.research.google.com/)


### ğŸ“Š **Diagram / Workflow Prompt**

<img src="https://github.com/codeandcrush089/ML-Models-Visualized/blob/main/Img/Lasso%20Regression.JPG" width="600">


### ğŸ“ˆ **Pros & Cons**

* âœ… Performs feature selection by setting coefficients to zero
* âœ… Prevents overfitting and improves model generalization
* âš ï¸ May remove useful correlated features unintentionally


### ğŸ“š **Real-World Use Case**

ğŸ’¡ Used in **genetic data analysis**, **marketing mix modeling**, and **sparse signal recovery** where only a few features drive predictions.

---

## 4 **Polynomial Regression**

### ğŸ“˜ Description

Polynomial Regression extends Linear Regression by modeling the relationship between input and target variables as an **nth-degree polynomial**. Itâ€™s useful when the data shows **non-linear trends** that a straight line canâ€™t capture.


### âš™ï¸ **Key Points**

* **Type:** Regression
* **Output Type:** Continuous
* **Algorithm / Technique:** Polynomial Feature Transformation + Linear Regression
* **Best Use Case:** Modeling **non-linear relationships** between independent and dependent variables.


### ğŸ’» **Code Example**

```python
from sklearn.preprocessing import PolynomialFeatures
from sklearn.linear_model import LinearRegression
poly = PolynomialFeatures(degree=2)
X_poly = poly.fit_transform(X)
model = LinearRegression().fit(X_poly, y)
print("RÂ² Score:", model.score(X_poly, y))
```

ğŸ‘‰ [Try it on Colab](https://colab.research.google.com/)


### ğŸ“Š **Diagram / Workflow Prompt**

<img src="https://github.com/codeandcrush089/ML-Models-Visualized/blob/main/Img/Polynomial%20Regression%201.JPG" width="600">

### ğŸ“ˆ **Pros & Cons**

* âœ… Captures non-linear relationships effectively
* âœ… Simple to implement and interpret up to moderate degrees
* âš ï¸ High-degree polynomials may cause **overfitting** and poor generalization


### ğŸ“š **Real-World Use Case**

ğŸ“ˆ Used in **growth curve modeling**, **temperature trend analysis**, and **demand forecasting** where patterns are **non-linear but continuous**.

---


